# @package _global_
project: "tuberculosis-tbx11k-final"
name: "tbfinal-11k-01-e100-1e-5"
img_size: 512
input_channels: 1
optimized_metric: valid/BinaryF1Score
max_epochs: 100

datamodule:
  _target_: trainer.datamodules.base.DataModuleTBX11K
  data_dir: '/data1/pxi-dataset/cxr/public/tbx11k'
  batch_size: 16
  stage: 'train'
  dataset_config:
    use_low_image: True
  
  transforms:
    train:
      resize:
        height: ${img_size}
        width: ${img_size}
        interpolation: 1
      random_flip:
        p: 0.5
      rand_aug_pixel:
        p: 0.5
        max_n: 3
        channel_stacking: False
        replicate: False
        num_channels: ${input_channels}
        train: True
        transforms:
          random_blur:
            blur_limit: 15
          random_gamma:
            gamma_limit: 20
          random_clahe:
            clip_limit: 4.0
          random_brightness:
            brightness_limit: 0.2
          random_contrast:
            contrast_limit: 0.2
          random_hist_equal:
            p: 0.5
          random_compression:
            quality_lower: 70
            quality_upper: 100
          random_noise:
            noise_limit: 0.1
          random_bilateral_filter:
            max_d: 5
            sigma_color: 10
            sigma_space: 10
          random_windowing:
            width_param: 4.0
            width_range: 2.0
            use_median: True
      random_spatial_augment_v2:
        n: 2
        height: ${img_size}
        width: ${img_size}
        scale: 0.1
    val:
      resize:
        height: ${img_size}
        width: ${img_size}
        interpolation: 1 # 1:linear, 2:bicubic
  
    test:
      resize:
        height: ${img_size}
        width: ${img_size}
        interpolation: 1 # 1:linear, 2:bicubic


model:
  _target_: trainer.models.BaseModel
  encoder:
    name: hrnet_w48
    out_indices : [1,2,3,4]
    in_chans: ${input_channels}
    features_only: True
  decoder:
    name: upsamplecatwithconv
    out_channels: 1024
  header:
    name: 2head_classifier
    num_classes2: 1 # normal, sick
    mid_channels: 1024
    return_logits: True


optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-5
  weight_decay: 1e-4

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${max_epochs}
  eta_min: 1.0e-06

engine:
  _target_: trainer.engines.TBClassification   

trainer:
  accelerator: "gpu"
  devices: [2]
  
  min_epochs: 1
  max_epochs: ${max_epochs}

  amp_backend: "native" 
  precision: 16 # auto mixed precsion

  gradient_clip_val : 0.5 # gradient cliping
  accumulate_grad_batches: 1 # gradient accumulation
  
  log_every_n_steps: 1
  num_sanity_val_steps: 2
  resume_from_checkpoint: null # ckpt path

          
hydra:
  run:
    dir: /data1/dongseopkim/experiment_logs/tb-final/${project}/${name}/${timestamp}
  sweep:
    dir: /data1/dongseopkim/experiment_logs/multiruns/${name}/${timestamp}
    subdir: ${hydra.job.num}

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: ${optimized_metric}
    mode: max
    save_top_k: 3
    save_last: false
    verbose: false
    dirpath: checkpoints/
    filename: epoch_{epoch:03d}
    auto_insert_metric_name: false
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: epoch


logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  project: ${project}
  name: ${name}
  save_dir: "."
  offline: False # set True to store all logs only locally
  id: null # pass correct id to resume experiment!
  # entity: ""  # set to name of your wandb team
  log_model: False
  prefix: ""
  job_type: "train"
  group: ""
  tags: []