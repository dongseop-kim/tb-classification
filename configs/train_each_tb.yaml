project: 'tb-classification'
name: 'tbx11k-02'
base_lr: 0.0001
max_epochs: 100
save_dir: '/data1/dongseopkim/${project}/${name}'

config_datamodule:
  data_dir: '/data1/dongseopkim/datasets/tbx11k'
  datasets: 'tbx11k'
  # data_dir: '/data1/dongseopkim/datasets/shenzhen'
  # datasets: 'shenzhen'
  num_workers: 0
  split_train: 'train' # default: 'train'
  split_val: 'val' # default: 'val'
  split_test: 'test' # default: 'test'
  additional_keys: [] # default: []
  
  # datasets_train: ['tbx11k'] 
  # datasets_val: ['tbx11k'] 
  # datasets_test: ['tbx11k'] 

  batch_size_train: 16
  batch_size_val: 8
  batch_size_test: 8


config_model:
  _target_: trainer.models.Model
  encoder:
    name: 'hrnet_w48'
    in_chans: 1
    out_indices: [1,2,3,4]
  decoder:
    name: 'upsample_concat'
    out_strides: 16
  header:
    name: 'tb_classifier_v1'
    


config_optimizer:
  _target_: torch.optim.RAdam
  lr: ${base_lr}

config_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${max_epochs}
  eta_min: 0.000001 # base_lr * 0.01

config_engine:
  _target_: trainer.engine.TBClsEngine

config_logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  project: ${project}
  name: ${name}
  save_dir: ${save_dir}
  offline: False
  id: null 
  log_model: False
  prefix: ""
  job_type: "train"
  group: ""
  tags: []


config_trainer:
  _target_:  pytorch_lightning.Trainer
  min_epochs: 1
  max_epochs: ${max_epochs}
  
  accelerator: 'gpu'
  devices: [2]
  precision: '16-mixed' # auto mixed precsion
  accumulate_grad_batches: 1 # gradient accumulation
  
  log_every_n_steps: 1
  num_sanity_val_steps: 2



  

