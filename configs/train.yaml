# @package _global_

# TODO: train.yaml 껍데기를 없애고, experiments 만 남겨 실험에 사용할 계획입니다.
#       template 이 필요 이상으로 겹겹이 복잡해서, 한곳에서 세팅을 거의 마칠 수 있게 개선하려 합니다.

defaults:
  - _self_
  # enable color logging
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog

# template-related: trainer/src/training_pipeline.py
print_config: True
ignore_warnings: False
train: True
test: False
seed: 0
log_savedir: "logs"

# defaults, but not long enough for making separate yamls
project: test
name: test
timestamp: ${now:%Y-%m-%d}_${now:%H-%M-%S}

hydra:
  run:
    dir: ${log_savedir}/${name}/${timestamp}
  sweep:
    dir: ${log_savedir}/multiruns/${name}/${timestamp}
    subdir: ${hydra.job.num}

trainer:
  _target_: pytorch_lightning.Trainer
  min_epochs: 1
  max_epochs: 200
  accelerator: gpu
  devices: [0]
  amp_backend: native # auto mixed precsion
  num_sanity_val_steps: 2
  log_every_n_steps: 1
  resume_from_checkpoint: null

callbacks:
  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: 1

  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar

  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: epoch
